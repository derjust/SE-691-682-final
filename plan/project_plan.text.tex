\documentclass{article}
\usepackage{arxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\graphicspath{ {./images/} }


\title{Evaluating the Efficacy of Multi-Task Attention Networks in Detecting Vulnerabilities in WASM Byte Code}

\date{June 14, 2023}

\author{Sebastian Just \\
	College of Computing \& Informatics\\
	Drexel University\\
	Philadelphia, PA 19104 \\
	\texttt{sj992@drexel.edu} \\
	\And
	Greg Morgan \\
	College of Computing \& Informatics\\
	Drexel University\\
	Philadelphia, PA 19104 \\
	\texttt{gm655@drexel.edu} \\
	\AND
	Frank Ottey \\
	College of Computing \& Informatics\\
	Drexel University\\
	Philadelphia, PA 19104 \\
	\texttt{fmo28@drexel.edu} \\
	\And
	Trevor Pawlewicz\\
	College of Computing \& Informatics\\
	Drexel University\\
	Philadelphia, PA 19104 \\
	\texttt{tmp365@drexel.com} \\
}

\renewcommand{\headeright}{}
\renewcommand{\undertitle}{}

\begin{document}
\maketitle

\begin{abstract}
The rise of ChatGPT and the power of large-language models, driven by the neural transformer architecture defined in the seminal paper by Vaswani et al. (Attention is All You Need) has garnered significant attention. While media focus has mainly been on the performance resulting from the application of this architecture to large language models in the natural language / text domain, the transformer architecture has proven effective in across a multitude of domains. The general attention mechanisms inherent in these models lend themselves well to solving problems in various domains through the encoding and decoding of domain-specific sequences. Our paper aims to extend the domain of recent advances in LLMs to computer byte code sequences via a fine-tuning process.

Our aim is to investigate the effectiveness of these fine-tuned, large-language, multi-task attention networks for detecting vulnerabilities in WASM byte code artifacts. Building on OpenAI's specialization of the "Whisper" model, which uses attention networks on audio sequences, we begin by focusing on specializing attention networks within large language models specifically for byte code sequences. Shifting from a generative to a discriminator model, we analyze and classify byte code sequences using labeled data. Our study involves fine-tuning existing large language models on WASM byte code sequences, generated by compiling vulnerable and non-vulnerable programs from multiple languages into a common WASM byte code format. This approach allows the network to learn underlying vulnerability patterns within byte code sequences, rather than language-specific sequence patterns. By producing properly labeled vulnerable and non-vulnerable byte code sequence data, we can ensure the integrity and quality of the training data.

To evaluate our approach, we intend to conduct comprehensive experiments on a benchmark dataset that we will create. We will then compare the performance of our model against existing vulnerability detection techniques and tools, using key metrics such as precision, recall, and F1-score to measure the quality of the model in identifying vulnerabilities. The goal will be to ensure that the benchmark dataset covers various common types of vulnerabilities encountered in untrusted code execution, such as memory corruption, code injection, and privilege escalation.

Our preliminary results are expected to demonstrate the potential of our approach in effectively identifying vulnerabilities in untrusted pre-compiled byte code artifacts intended for execution in unknown environments. This research advances automated vulnerability detection techniques, enhancing software security and enabling proactive mitigation strategies. Potential stakeholders interested in this research include the software security industry, web browser engine authors, and virtual machine authors. Further extensions of this research could include automated, byte code level program repair, useful for when access to source code is limited, automated bug detection, or even AI-driven byte code optimization.
\end{abstract}

\keywords{Language models \and Byte code analysis \and Software security \and WASM (WebAssembly)}

\section{Data and Dataset Selection}
Our aim is to extend the recent advancements demonstrated by highly effective multi-task attention network architectures in language modeling to the application of byte code analysis. Specifically WebAssembly (WASM) byte code analysis. To achieve this, a benchmark corpus of labeled training data needs to be generated.

WASM was selected as a suitable byte code candidate due to its significance as a next-generation, language-agnostic, client-side virtual machine target for program execution. Given its role in executing untrusted code on consumer-owned devices, the ability to automatically analyze and detect vulnerabilities in untrusted programs before execution on client hardware holds immense value in the domain of software security. It is crucial to generate byte code sequences from a diverse range of higher-level programming languages that compile down to WASM byte code, as this will provide a valuable dataset for analysis.

In addition to ensuring that byte code sequence examples are collected from a variety of programming languages, we commit to labeling the resulting byte code sequences regarding their disposition as either vulnerable or non-vulnerable code. We will aim to ensure that several classes of vulnerability are represented amongst the training data as well.

\section{Model Architecture and Fine-tuning}
Our approach regarding leveraging draws inspiration from previous work, specifically OpenAI's "Whisper" model, where the transformer architecture was successfully applied to audio sequences.

The machine learning algorithm for this research would involve adapting the transformer architecture to the domain of byte code analysis. The transformer architecture consists of stacked self-attention and feed-forward layers. Self-attention mechanisms allow the model to attend to different parts of the input sequence, capturing contextual dependencies effectively. The feed-forward layers provide non-linear transformations to further process the representations.

In the case of byte code analysis, the transformer-based model would take byte code sequences as input and encode them into vector embeddings. These vector embeddings would ideally capture semantic information and maintain similarity between related byte code sequences. By training the model on a diverse dataset of labeled vulnerability data, the model learns to discriminate between vulnerable and non-vulnerable code based on the embeddings.

To determine the similarity between programs written in different languages, the model's vector embeddings of common byte code sequences can be compared in the shared vector space. This enables the model to measure the semantic similarity between programs, even if they are written in different languages. The vector embeddings provide a compact representation that encapsulates the relevant information about the byte code sequences and allows for efficient computation of similarity metrics.

The machine learning algorithm leverages the power of the transformer architecture to analyze and represent byte code sequences, enabling the identification of vulnerabilities and the assessment of semantic similarity across different programs and languages.

\section{Evaluation Metrics}
Some evaluation metrics that we are looking to utilize include precision, recall, and the F1-Score. Precision measures the rate of true positives, for example, in our case, correctly detected vulnerable byte code sequences, among all items classified as positives, which would include byte code sequences classified as vulnerable that actually were not. A higher precision score would indicate the model is good at determining the difference between vulnerable and non-vulnerable byte code sequences.

Recall, on the other hand, also known as sensitivity or true positive rate, measures the proportion of true positives that are correctly detected among all actual vulnerabilities present in the dataset. A high recall indicates the ability to identify a significant portion of vulnerabilities, minimizing false negatives (missed vulnerabilities).

F1-Score: The F1-score is the harmonic mean of precision and recall, providing a balanced assessment of both metrics. It offers an overall measure of the model's performance by considering both false positives and false negatives. A higher F1-score indicates a better trade-off between precision and recall.

\section{Comparative Analysis}
There has been some prior research in this area, as evidenced by the existing literature in our references section. However, our research introduces several novel elements in our experimentation.

Firstly, our exploration of WebAssembly (WASM) as a byte code target for language modeling is relatively new. WASM has gained popularity due to the growing need to run untrusted code on the web within sandboxed environments. While sandboxing provides some level of security, no environment is completely isolated. By improving the trustworthiness of untrusted code through pre-execution analysis via a language model, we aim to improve overall security.

Another aspect that contributes to the novelty of our research is the use of a dataset comprising bytecode sequences generated from a diverse set of programming languages. Existing studies in this field have primarily focused on specific languages and specific problems. In contrast, our objective is to enable the model to operate more generally on byte code, without being limited to recognizing patterns specific to a particular programming language.

\section{Implications and Future Work}
By establishing that neural language models built from attention networks can work with byte code sequences as well as they work with other sequences, we might be able to leverage these systems to perform automated security scanning of untrusted byte code streams, or allow us the measure the semantic similarity of different byte code sequences (programs)

One particular area of research that could be interesting is to see if we could build AI systems to use their knowledge of bytecode sequences to perform optimizations on bytecode sequences.

\section{Project Plan}

In order to meet our objectives, we will need to propose a fairly ambitious timeline.

\subsection{Week 1: Project Setup and Literature Review}
\begin{itemize}
\item Establish the project environment by setting up the required software and tools necessary for conducting the research on vulnerability detection in WASM byte code artifacts, including the installation and configuration of relevant frameworks, libraries, and development environments.
\item Conduct an in-depth literature review on vulnerability detection, attention networks, and WASM byte code analysis. 
\item Refine the research questions and objectives based on the literature review. 
\end{itemize}

\subsection{Week 2: Data Collection and Preprocessing}
\begin{itemize}
\item Identify and gather a diverse set of vulnerable and non-vulnerable WASM byte code artifacts. 
\item Preprocess the collected data, ensuring proper labeling and formatting for training the models.
\end{itemize}

\subsection{Week 3-5: Language Model Adaptation and Training}
\begin{itemize}
\item Choose a suitable large language model as the base architecture. 
\item Modify the model to specialize in byte code sequence analysis instead of natural text. 
\item Implement the necessary modifications to enable multi-task learning for vulnerability detection. 
\item Train the adapted model using the labeled WASM byte code sequences. 
\item Define appropriate loss functions and evaluation metrics for the training process. 
\item Conduct multiple training iterations, monitoring and recording the model's performance.
\end{itemize}

\subsection{Week 6-7: Model Evaluation and Refinement}
\begin{itemize}
\item Evaluate the model on a validation dataset to assess its performance. 
\item Measure key metrics such as precision, recall, and F1-score to determine the accuracy of vulnerability detection. 
\item Analyze the performance of the model and identify areas for improvement. 
\item Refine the model further by adjusting hyperparameters, loss functions, or incorporating additional training data if necessary. 
\item Repeat the evaluation process to measure the impact of the refinements. 
\end{itemize}

\subsection{Week 8-9: Results Analysis and Conclusion/Future Works}
\begin{itemize}
\item Evaluate the performance of the model on the test dataset. 
\item Compare the model's results against existing vulnerability detection techniques and tools. 
\item Identify strengths, weaknesses, and potential areas of further improvement for our approach. 
\item Interpret the findings and discuss the implications of our approach in detecting vulnerabilities in WASM byte code artifacts. 
\item Based on results, determine potential areas of future work. 
\end{itemize}

\subsection{Week 10: Report Writing and Finalization}
\begin{itemize}
\item Compile all the findings, methodology, and analysis into a comprehensive research paper. 
\item Write the introduction, methodology, results, discussion, and conclusion sections. 
\item Revise and proofread the paper. 
\end{itemize}


\nocite{Eth2Vec}
\nocite{DNNForJS}
\nocite{SmartContracts}
\nocite{NueralModel}
\nocite{MethodCrash}
\nocite{TokenizingCode}
\nocite{BigVocabNot}
\nocite{Attention}
\nocite{OpenAIWhisper}
\nocite{BERTWebShell}


\bibliographystyle{plainnat}
\bibliography{refs} 


\end{document}
